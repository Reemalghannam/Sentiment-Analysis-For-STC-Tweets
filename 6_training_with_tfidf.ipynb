# -*- coding: utf-8 -*-
"""6_Training with tfidf.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15grxdGkh0POud07T-Wbng1DvCOeSBHhC

Students should write a more consise code.
The detailed coding here is for demonstration.
"""

#SVM and LR 1: Tf-idf with features 2: Tf-idf without features 3: Aravec with features 4: Aravec without features

#pip install sklearn-pandas

import pandas as pd
from sklearn import preprocessing
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
from sklearn_pandas import DataFrameMapper
from sklearn.svm import LinearSVC

from google.colab import files

uploaded = files.upload()

# use only the first columns
df = pd.read_excel("dataallFINAL.xlsx", usecols=lambda col: col not in ["id","Unnamed: 0","Unnamed: 0.1","Unnamed: 0.2"])
dfdesc = pd.read_excel("dataallFINAL.xlsx", usecols=lambda col: col not in ["hashtag","url","number","id","Unnamed: 0","Unnamed: 0.1","Unnamed: 0.2"])
#or df.drop([["id","Unnamed: 0","Unnamed: 0.1","Unnamed: 0.2"]], axis=1)

dfdesc.head(5)

#basic descriptive stats (project note)
dfdesc.describe().T

dfdesc.groupby('class').describe().T

y = dfdesc["class"]
y.value_counts()

#we need to encode the class labels in the target variable as numbers to ensure compatibility with some models in Scikit-learn. Because we have binary classes, let's use LabelEncoder and set 'spam' = 1 and 'not' = 0.
le = preprocessing.LabelEncoder()
y_enc = le.fit_transform(y)
#print(y_enc)

vect = TfidfVectorizer()

X_train,X_test,Y_train,Y_test = train_test_split(df["tweet"].values.astype('U'),y_enc, test_size = 0.2, random_state = 10) #80/20 training and test set split. random state Controls the shuffling applied to the data before applying the split. 
vect.fit(X_train)
X_train_df = vect.transform(X_train)
X_test_df = vect.transform(X_test)
type(X_train_df)

"""# **Spam Detection Using Logistic Regression and Tf-idf**"""

#Machine Learning (classification) LogisticRegression
LRmodel = LogisticRegression()
LRmodel.fit(X_train_df,Y_train)

# Making predictions

prediction = dict()

prediction["Logistic"] = LRmodel.predict(X_test_df)
print(prediction["Logistic"])

# Reviewing the metrics
print ("Accuracy:",accuracy_score(Y_test,prediction["Logistic"]))

print(classification_report(Y_test,prediction["Logistic"]))
#macro average (Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.), 
#weighted average (Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). 
#This alters ‘macro’ to account for label imbalance)

"""# **Spam Detection Using Logistic Regression and Tf-idf + Other Features**"""

#Add the features of the dataframe that you want to transform and/or combine
df['tweet']=df['tweet'].values.astype('U') #in case ValueError: np.nan is an invalid document
mapper = DataFrameMapper([
     ("tweet", TfidfVectorizer()),
     ("hashtag", None),
     ("url", None),
     ("number", None)
 ])

"""
Use the fit_transform method to transform the old dataframe into a new one
that can be fed to the machine learning algorithm.
"""
features = mapper.fit_transform(df)
 
# Split the data between train and test
x, x_test, y, y_test = train_test_split(features,y_enc,test_size=0.2,train_size=0.8, random_state = 10)
 
LRmodel2 = LogisticRegression()
LRmodel2.fit(x,y)

prediction = dict()

prediction["Logistic2"] = LRmodel2.predict(x_test)
print(prediction["Logistic2"])
print ("Accuracy:",accuracy_score(y_test,prediction["Logistic2"]))
print(classification_report(y_test,prediction["Logistic2"]))

"""# **Spam Detection Using Support Vector Machine (SVM) and Tf-idf**"""

svm = LinearSVC().fit(X_train_df,Y_train)

prediction["SVM"] = svm.predict(X_test_df)
print(prediction["SVM"])

accuracy_score(Y_test,prediction["SVM"])

print(classification_report(Y_test,prediction["SVM"]))

"""# **Spam Detection Using Support Vector Machine (SVM) and Tf-idf + Other Features**"""

svm2 = LinearSVC().fit(x,y)

prediction["SVM2"] = svm2.predict(x_test)
print(prediction["SVM2"])
print ("Accuracy:",accuracy_score(y_test,prediction["SVM2"]))
print(classification_report(y_test,prediction["SVM2"]))

"""# **Naive Bayes (just for demo)**"""

#Machine Learning (classification) Naive Bayes
from sklearn.naive_bayes import MultinomialNB
NBmodel = MultinomialNB().fit(X_train_df,Y_train)

prediction["Naive"] = NBmodel.predict(X_test_df)
print(prediction["Naive"])

accuracy_score(y_test,prediction["Naive"])

print(classification_report(y_test,prediction["Naive"]))

pd.DataFrame(
    confusion_matrix(y_test, prediction["Naive"]),
    index=[['actual', 'actual'], ['not', 'spam']],
    columns=[['predicted', 'predicted'], ['not', 'spam']]
)

#print('predicted:', NBmodel.predict(X_test_df)[0])
#print('expected :', df["class"][0])

#Train the classifier on the whole dataset and take a look at the top 20 n-grams that are most predictive of spam.
NBmodelfull = MultinomialNB().fit(vect.fit_transform(df["tweet"]),y_enc)
pd.Series(
    NBmodelfull.coef_.T.ravel(),
    index=vect.get_feature_names()
).sort_values(ascending=False)[:20]

#a function that'll decide whether a string is spam or not, and apply it on a hypothetical message 
def spam_filter(message):
    if NBmodelfull.predict(vect.transform([message])):
        return 'spam'
    else:
        return 'not spam'

print(spam_filter('حلو مره'))
print(spam_filter('زين ممتاز'))
print(spam_filter('نزتاز http:\\'))