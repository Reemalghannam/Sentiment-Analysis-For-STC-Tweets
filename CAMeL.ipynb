{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAMeL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMQa4-rtA7Sy"
      },
      "source": [
        "Reference: https://colab.research.google.com/drive/1Y3qCbD6Gw1KEw-lixQx1rI6WlyWnrnDS?usp=sharing#scrollTo=U7JXVSeA8-rr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o1ghwLO-Du0"
      },
      "source": [
        "## **Installation and Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jftUSMWt9z73",
        "outputId": "270824a4-e3f5-4eb2-beeb-f4c27ae48ef7"
      },
      "source": [
        "%pip install camel-tools"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting camel-tools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/23/331ce904926a8d53a527aac34bfe03fffb9fd1d4597924cbcd65432a53ef/camel_tools-1.1.0.tar.gz (56kB)\n",
            "\r\u001b[K     |█████▉                          | 10kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 20kB 15.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 30kB 20.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 40kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 51kB 16.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.15.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.6.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.7/dist-packages (from camel-tools) (4.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.22.2.post1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.3.3)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.7.1+cu101)\n",
            "Collecting transformers==3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 12.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from camel-tools) (2.23.0)\n",
            "Collecting camel-kenlm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/4e/147d258c7168b8f538e6aa7c4dc602b2bb696452502af8af35876a28de78/camel-kenlm-2020.11.2.tar.gz (250kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 17.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->camel-tools) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->camel-tools) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->camel-tools) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->camel-tools) (3.7.4.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->camel-tools) (2019.12.20)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 35.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->camel-tools) (3.0.12)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/59/68c7e3833f535615fb97d33ffcb7b30bbf62bc7477a9c59cd19ad8535d72/tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 43.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->camel-tools) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->camel-tools) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 30.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->camel-tools) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->camel-tools) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->camel-tools) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->camel-tools) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2->camel-tools) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2->camel-tools) (7.1.2)\n",
            "Building wheels for collected packages: camel-tools, camel-kenlm, sacremoses\n",
            "  Building wheel for camel-tools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for camel-tools: filename=camel_tools-1.1.0-cp37-none-any.whl size=96899 sha256=70e561c7e74807cd5c5956b39c8f1ee491e2fff0c9b412bf1dea8c8b3c3a6751\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/8d/46/949198daef42f9e3bf64ed645a4af05816f47aa7677ac74589\n",
            "  Building wheel for camel-kenlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for camel-kenlm: filename=camel_kenlm-2020.11.2-cp37-cp37m-linux_x86_64.whl size=2313765 sha256=1d9a566cff03acd6b0aab84c1bfdfec1485382a99268011031160e2eb587996f\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/df/36/46723b5cb1a11c345ad10813f13d2ce2452e95b0aaca7ad332\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=aeefbe11a180ff8dd31c3ae7e504ec9b0880f43617d33fc88534ce3aacb353ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built camel-tools camel-kenlm sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers, camel-kenlm, camel-tools\n",
            "Successfully installed camel-kenlm-2020.11.2 camel-tools-1.1.0 sacremoses-0.0.43 sentencepiece-0.1.95 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBVEBqYR-PkS",
        "outputId": "8ee2de8e-9584-435b-d8c2-9ca3ded6cdcc"
      },
      "source": [
        "#Mount drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "%mkdir /gdrive/MyDrive/camel_tools"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvtIlpFI_A2U"
      },
      "source": [
        "#Install the data\n",
        "os.environ['CAMELTOOLS_DATA'] = '/gdrive/MyDrive/camel_tools'\n",
        "\n",
        "!export | camel_data full #light or or full"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs8QLxtp_gG2"
      },
      "source": [
        "#If it was not the first time...\n",
        "#%pip install camel-tools\n",
        "\n",
        "#from google.colab import drive\n",
        "#import os\n",
        "\n",
        "#drive.mount('/gdrive')\n",
        "#os.environ['CAMELTOOLS_DATA'] = '/gdrive/MyDrive/camel_tools'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYgBjIDNFFXB",
        "outputId": "a984a1f8-003f-4307-852b-7800c93bbb94"
      },
      "source": [
        "#Space and punctuation splitting التقسيم على الفراغات وعلامات الترقيم\n",
        "#The simple_word_tokenize function will convert any text to its tokenized list.\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "sentence= \"هل ذهبت إلى المكتبة؟\"\n",
        "print (sentence)\n",
        "\n",
        "sent_split= simple_word_tokenize(sentence)\n",
        "print (sent_split)\n",
        "\n",
        "#split() method to tokenize words by whitespace but it doesn't seperate punctuation from words"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "هل ذهبت إلى المكتبة؟\n",
            "['هل', 'ذهبت', 'إلى', 'المكتبة', '؟']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xd4WNzTzxwFl",
        "outputId": "39b8719c-32ba-4042-fa34-9e11bcba0f11"
      },
      "source": [
        "#Unicode Normalization تطبيع اليونيكود\n",
        "from camel_tools.utils.normalize import normalize_unicode\n",
        "sentence= \"ﷺ\"\n",
        "print (sentence)\n",
        "\n",
        "sent_norm= normalize_unicode(sentence)\n",
        "print (sent_norm)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ﷺ\n",
            "صلى الله عليه وسلم\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqbXOnWN0aeJ",
        "outputId": "e5b6e8b8-6493-408d-a322-dee07b087b10"
      },
      "source": [
        "#Orthographic Normalization التنميط الإملائي\n",
        "from camel_tools.utils.normalize import normalize_alef_maksura_ar #Without needing to use regex nor indicating what letter to replace with what other letter, thereby consistency in datasets\n",
        "from camel_tools.utils.normalize import normalize_alef_ar\n",
        "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
        "#from camel_tools.utils.normalize import n\n",
        "\n",
        "sentence= \"هل ذهبت إلى المكتبة؟\"\n",
        "print (sentence)\n",
        "\n",
        "sent_norm2= normalize_alef_ar(sentence)\n",
        "sent_norm2= normalize_alef_maksura_ar(sent_norm2)\n",
        "sent_norm2=normalize_teh_marbuta_ar(sent_norm2)\n",
        "print(sent_norm2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "هل ذهبت إلى المكتبة؟\n",
            "هل ذهبت الي المكتبه؟\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HKomU90zCXE",
        "outputId": "551a1ade-4d7c-4420-9503-2e386d0f0899"
      },
      "source": [
        "#Diacritization إزالة التشكيل\n",
        "from camel_tools.utils.dediac import  dediac_ar\n",
        "sentence= \"أَنَا اسمِي نورَة\"\n",
        "print (sentence)\n",
        "\n",
        "sent_dediac= dediac_ar(sentence)\n",
        "print (sent_dediac)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "أَنَا اسمِي نورَة\n",
            "أنا اسمي نورة\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaH_Ttae4kLp",
        "outputId": "8019f700-fe19-40ae-d56b-7e1a2c8444d7"
      },
      "source": [
        "#Morphological Analysis التحليل الصرفي(this is where you can extract stems and lemmas)\n",
        "from camel_tools.morphology.database import MorphologyDB\n",
        "from camel_tools.morphology.analyzer import Analyzer\n",
        "\n",
        "# First, we need to load a morphological database.\n",
        "# Here, we load the default database which is used for analyzing\n",
        "# Modern Standard Arabic. \n",
        "db = MorphologyDB.builtin_db()\n",
        "\n",
        "analyzer = Analyzer(db)\n",
        "\n",
        "analyses = analyzer.analyze('موظف')\n",
        "\n",
        "for analysis in analyses:\n",
        "    print(analysis, '\\n')\n",
        "\n",
        "#Morphological analysis is the process of generating all possible readings (analyses) of a given word out of context. \n",
        "#All analyses are generated from the undiacritized form of the input word. \n",
        "#Each of these analyses is defined by a set lexical and morphological features.\n",
        "#https://camel-tools.readthedocs.io/en/latest/reference/camel_morphology_features.html"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'diac': 'مُوَظَّف', 'lex': 'مُوَظَّف_2', 'bw': 'مُوَظَّف/ADJ', 'gloss': 'employed;hired', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'u', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'pattern': 'مُوَ2َّ3', 'root': '#.ظ.ف', 'catib6': 'NOM', 'ud': 'ADJ', 'd1seg': 'مُوَظَّف', 'd1tok': 'مُوَظَّف', 'atbseg': 'مُوَظَّف', 'd3seg': 'مُوَظَّف', 'd2seg': 'مُوَظَّف', 'd2tok': 'مُوَظَّف', 'atbtok': 'مُوَظَّف', 'd3tok': 'مُوَظَّف', 'bwtok': 'مُوَظَّف', 'pos_lex_logprob': -5.400551, 'caphi': 'm_u_w_a_dh._dh._a_f', 'pos_logprob': -0.9868824, 'gen': 'm', 'lex_logprob': -5.400551, 'num': 's', 'stem': 'مُوَظَّف', 'stemgloss': 'employed;hired', 'stemcat': 'N-ap'} \n",
            "\n",
            "{'diac': 'مُوَظَّفِ', 'lex': 'مُوَظَّف_2', 'bw': 'مُوَظَّف/ADJ+ِ/CASE_DEF_GEN', 'gloss': 'employed;hired+[def.gen.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'g', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'pattern': 'مُوَ2َّ3ِ', 'root': '#.ظ.ف', 'catib6': 'NOM', 'ud': 'ADJ', 'd1seg': 'مُوَظَّفِ', 'd1tok': 'مُوَظَّفِ', 'atbseg': 'مُوَظَّفِ', 'd3seg': 'مُوَظَّفِ', 'd2seg': 'مُوَظَّفِ', 'd2tok': 'مُوَظَّفِ', 'atbtok': 'مُوَظَّفِ', 'd3tok': 'مُوَظَّفِ', 'bwtok': 'مُوَظَّف_+ِ', 'pos_lex_logprob': -5.400551, 'caphi': 'm_u_w_a_dh._dh._a_f_i', 'pos_logprob': -0.9868824, 'gen': 'm', 'lex_logprob': -5.400551, 'num': 's', 'stem': 'مُوَظَّف', 'stemgloss': 'employed;hired', 'stemcat': 'N-ap'} \n",
            "\n",
            "{'diac': 'مُوَظَّفَ', 'lex': 'مُوَظَّف_2', 'bw': 'مُوَظَّف/ADJ+َ/CASE_DEF_ACC', 'gloss': 'employed;hired+[def.acc.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'a', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'pattern': 'مُوَ2َّ3َ', 'root': '#.ظ.ف', 'catib6': 'NOM', 'ud': 'ADJ', 'd1seg': 'مُوَظَّفَ', 'd1tok': 'مُوَظَّفَ', 'atbseg': 'مُوَظَّفَ', 'd3seg': 'مُوَظَّفَ', 'd2seg': 'مُوَظَّفَ', 'd2tok': 'مُوَظَّفَ', 'atbtok': 'مُوَظَّفَ', 'd3tok': 'مُوَظَّفَ', 'bwtok': 'مُوَظَّف_+َ', 'pos_lex_logprob': -5.400551, 'caphi': 'm_u_w_a_dh._dh._a_f_a', 'pos_logprob': -0.9868824, 'gen': 'm', 'lex_logprob': -5.400551, 'num': 's', 'stem': 'مُوَظَّف', 'stemgloss': 'employed;hired', 'stemcat': 'N-ap'} \n",
            "\n",
            "{'diac': 'مُوَظَّفُ', 'lex': 'مُوَظَّف_2', 'bw': 'مُوَظَّف/ADJ+ُ/CASE_DEF_NOM', 'gloss': 'employed;hired+[def.nom.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'n', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'pattern': 'مُوَ2َّ3ُ', 'root': '#.ظ.ف', 'catib6': 'NOM', 'ud': 'ADJ', 'd1seg': 'مُوَظَّفُ', 'd1tok': 'مُوَظَّفُ', 'atbseg': 'مُوَظَّفُ', 'd3seg': 'مُوَظَّفُ', 'd2seg': 'مُوَظَّفُ', 'd2tok': 'مُوَظَّفُ', 'atbtok': 'مُوَظَّفُ', 'd3tok': 'مُوَظَّفُ', 'bwtok': 'مُوَظَّف_+ُ', 'pos_lex_logprob': -5.400551, 'caphi': 'm_u_w_a_dh._dh._a_f_u', 'pos_logprob': -0.9868824, 'gen': 'm', 'lex_logprob': -5.400551, 'num': 's', 'stem': 'مُوَظَّف', 'stemgloss': 'employed;hired', 'stemcat': 'N-ap'} \n",
            "\n",
            "{'diac': 'مُوَظَّفٌ', 'lex': 'مُوَظَّف_2', 'bw': 'مُوَظَّف/ADJ+ٌ/CASE_INDEF_NOM', 'gloss': 'employed;hired+[indef.nom.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'n', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'pattern': 'مُوَ2َّ3ٌ', 'root': '#.ظ.ف', 'catib6': 'NOM', 'ud': 'ADJ', 'd1seg': 'مُوَظَّفٌ', 'd1tok': 'مُوَظَّفٌ', 'atbseg': 'مُوَظَّفٌ', 'd3seg': 'مُوَظَّفٌ', 'd2seg': 'مُوَظَّفٌ', 'd2tok': 'مُوَظَّفٌ', 'atbtok': 'مُوَظَّفٌ', 'd3tok': 'مُوَظَّفٌ', 'bwtok': 'مُوَظَّف_+ٌ', 'pos_lex_logprob': -5.400551, 'caphi': 'm_u_w_a_dh._dh._a_f_u_n', 'pos_logprob': -0.9868824, 'gen': 'm', 'lex_logprob': -5.400551, 'num': 's', 'stem': 'مُوَظَّف', 'stemgloss': 'employed;hired', 'stemcat': 'N-ap'} \n",
            "\n",
            "{'diac': 'مُوَظَّفٍ', 'lex': 'مُوَظَّف_2', 'bw': 'مُوَظَّف/ADJ+ٍ/CASE_INDEF_GEN', 'gloss': 'employed;hired+[indef.gen.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'g', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'pattern': 'مُوَ2َّ3ٍ', 'root': '#.ظ.ف', 'catib6': 'NOM', 'ud': 'ADJ', 'd1seg': 'مُوَظَّفٍ', 'd1tok': 'مُوَظَّفٍ', 'atbseg': 'مُوَظَّفٍ', 'd3seg': 'مُوَظَّفٍ', 'd2seg': 'مُوَظَّفٍ', 'd2tok': 'مُوَظَّفٍ', 'atbtok': 'مُوَظَّفٍ', 'd3tok': 'مُوَظَّفٍ', 'bwtok': 'مُوَظَّف_+ٍ', 'pos_lex_logprob': -5.400551, 'caphi': 'm_u_w_a_dh._dh._a_f_i_n', 'pos_logprob': -0.9868824, 'gen': 'm', 'lex_logprob': -5.400551, 'num': 's', 'stem': 'مُوَظَّف', 'stemgloss': 'employed;hired', 'stemcat': 'N-ap'} \n",
            "\n",
            "{'diac': 'مُوَظَّف', 'lex': 'مُوَظَّف_1', 'bw': 'مُوَظَّف/NOUN', 'gloss': 'employee', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'u', 'enc0': '0', 'rat': 'r', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'pattern': 'مُوَ2َّ3', 'root': '#.ظ.ف', 'catib6': 'NOM', 'ud': 'NOUN', 'd1seg': 'مُوَظَّف', 'd1tok': 'مُوَظَّف', 'atbseg': 'مُوَظَّف', 'd3seg': 'مُوَظَّف', 'd2seg': 'مُوَظَّف', 'd2tok': 'مُوَظَّف', 'atbtok': 'مُوَظَّف', 'd3tok': 'مُوَظَّف', 'bwtok': 'مُوَظَّف', 'pos_lex_logprob': -3.747339, 'caphi': 'm_u_w_a_dh._dh._a_f', 'pos_logprob': -0.4344233, 'gen': 'm', 'lex_logprob': -3.747339, 'num': 's', 'stem': 'مُوَظَّف', 'stemgloss': 'employee', 'stemcat': 'Nall'} \n",
            "\n",
            "{'diac': 'مُوَظَّفِ', 'lex': 'مُوَظَّف_1', 'bw': 'مُوَظَّف/NOUN+ِ/CASE_DEF_GEN', 'gloss': 'employee+[def.gen.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'g', 'enc0': '0', 'rat': 'r', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'pattern': 'مُوَ2َّ3ِ', 'root': '#.ظ.ف', 'catib6': 'NOM', 'ud': 'NOUN', 'd1seg': 'مُوَظَّفِ', 'd1tok': 'مُوَظَّفِ', 'atbseg': 'مُوَظَّفِ', 'd3seg': 'مُوَظَّفِ', 'd2seg': 'مُوَظَّفِ', 'd2tok': 'مُوَظَّفِ', 'atbtok': 'مُوَظَّفِ', 'd3tok': 'مُوَظَّفِ', 'bwtok': 'مُوَظَّف_+ِ', 'pos_lex_logprob': -3.747339, 'caphi': 'm_u_w_a_dh._dh._a_f_i', 'pos_logprob': -0.4344233, 'gen': 'm', 'lex_logprob': -3.747339, 'num': 's', 'stem': 'مُوَظَّف', 'stemgloss': 'employee', 'stemcat': 'Nall'} \n",
            "\n",
            "{'diac': 'مُوَظَّفَ', 'lex': 'مُوَظَّف_1', 'bw': 'مُوَظَّف/NOUN+َ/CASE_DEF_ACC', 'gloss': 'employee+[def.acc.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'a', 'enc0': '0', 'rat': 'r', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'pattern': 'مُوَ2َّ3َ', 'root': '#.ظ.ف', 'catib6': 'NOM', 'ud': 'NOUN', 'd1seg': 'مُوَظَّفَ', 'd1tok': 'مُوَظَّفَ', 'atbseg': 'مُوَظَّفَ', 'd3seg': 'مُوَظَّفَ', 'd2seg': 'مُوَظَّفَ', 'd2tok': 'مُوَظَّفَ', 'atbtok': 'مُوَظَّفَ', 'd3tok': 'مُوَظَّفَ', 'bwtok': 'مُوَظَّف_+َ', 'pos_lex_logprob': -3.747339, 'caphi': 'm_u_w_a_dh._dh._a_f_a', 'pos_logprob': -0.4344233, 'gen': 'm', 'lex_logprob': -3.747339, 'num': 's', 'stem': 'مُوَظَّف', 'stemgloss': 'employee', 'stemcat': 'Nall'} \n",
            "\n",
            "{'diac': 'مُوَظَّفُ', 'lex': 'مُوَظَّف_1', 'bw': 'مُوَظَّف/NOUN+ُ/CASE_DEF_NOM', 'gloss': 'employee+[def.nom.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'n', 'enc0': '0', 'rat': 'r', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'pattern': 'مُوَ2َّ3ُ', 'root': '#.ظ.ف', 'catib6': 'NOM', 'ud': 'NOUN', 'd1seg': 'مُوَظَّفُ', 'd1tok': 'مُوَظَّفُ', 'atbseg': 'مُوَظَّفُ', 'd3seg': 'مُوَظَّفُ', 'd2seg': 'مُوَظَّفُ', 'd2tok': 'مُوَظَّفُ', 'atbtok': 'مُوَظَّفُ', 'd3tok': 'مُوَظَّفُ', 'bwtok': 'مُوَظَّف_+ُ', 'pos_lex_logprob': -3.747339, 'caphi': 'm_u_w_a_dh._dh._a_f_u', 'pos_logprob': -0.4344233, 'gen': 'm', 'lex_logprob': -3.747339, 'num': 's', 'stem': 'مُوَظَّف', 'stemgloss': 'employee', 'stemcat': 'Nall'} \n",
            "\n",
            "{'diac': 'مُوَظَّفٌ', 'lex': 'مُوَظَّف_1', 'bw': 'مُوَظَّف/NOUN+ٌ/CASE_INDEF_NOM', 'gloss': 'employee+[indef.nom.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'n', 'enc0': '0', 'rat': 'r', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'pattern': 'مُوَ2َّ3ٌ', 'root': '#.ظ.ف', 'catib6': 'NOM', 'ud': 'NOUN', 'd1seg': 'مُوَظَّفٌ', 'd1tok': 'مُوَظَّفٌ', 'atbseg': 'مُوَظَّفٌ', 'd3seg': 'مُوَظَّفٌ', 'd2seg': 'مُوَظَّفٌ', 'd2tok': 'مُوَظَّفٌ', 'atbtok': 'مُوَظَّفٌ', 'd3tok': 'مُوَظَّفٌ', 'bwtok': 'مُوَظَّف_+ٌ', 'pos_lex_logprob': -3.747339, 'caphi': 'm_u_w_a_dh._dh._a_f_u_n', 'pos_logprob': -0.4344233, 'gen': 'm', 'lex_logprob': -3.747339, 'num': 's', 'stem': 'مُوَظَّف', 'stemgloss': 'employee', 'stemcat': 'Nall'} \n",
            "\n",
            "{'diac': 'مُوَظَّفٍ', 'lex': 'مُوَظَّف_1', 'bw': 'مُوَظَّف/NOUN+ٍ/CASE_INDEF_GEN', 'gloss': 'employee+[indef.gen.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'g', 'enc0': '0', 'rat': 'r', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'pattern': 'مُوَ2َّ3ٍ', 'root': '#.ظ.ف', 'catib6': 'NOM', 'ud': 'NOUN', 'd1seg': 'مُوَظَّفٍ', 'd1tok': 'مُوَظَّفٍ', 'atbseg': 'مُوَظَّفٍ', 'd3seg': 'مُوَظَّفٍ', 'd2seg': 'مُوَظَّفٍ', 'd2tok': 'مُوَظَّفٍ', 'atbtok': 'مُوَظَّفٍ', 'd3tok': 'مُوَظَّفٍ', 'bwtok': 'مُوَظَّف_+ٍ', 'pos_lex_logprob': -3.747339, 'caphi': 'm_u_w_a_dh._dh._a_f_i_n', 'pos_logprob': -0.4344233, 'gen': 'm', 'lex_logprob': -3.747339, 'num': 's', 'stem': 'مُوَظَّف', 'stemgloss': 'employee', 'stemcat': 'Nall'} \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv04A367BzkP",
        "outputId": "355fdeaa-8d62-48b1-cb2b-b497a4e6a2b6"
      },
      "source": [
        "#POS\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "from camel_tools.tagger.default import DefaultTagger\n",
        "\n",
        "mle = MLEDisambiguator.pretrained()\n",
        "tagger = DefaultTagger(mle, 'pos')\n",
        "\n",
        "# The tagger expects pre-tokenized text\n",
        "sentence = simple_word_tokenize('نجح بايدن في الانتخابات')\n",
        "\n",
        "pos_tags = tagger.tag(sentence)\n",
        "\n",
        "print(pos_tags)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['verb', 'noun_prop', 'prep', 'noun']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovr_ITEHCIhd",
        "outputId": "4c68a0eb-8ee8-48e8-8857-e313c1dc1453"
      },
      "source": [
        "#Morphological Tokenization \n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
        "\n",
        "# The tokenizer expects pre-tokenized text\n",
        "sentence = simple_word_tokenize('فتنفست الصعداء')\n",
        "print(sentence)\n",
        "\n",
        "# Load a pretrained disambiguator to use with a tokenizer\n",
        "mle = MLEDisambiguator.pretrained('calima-msa-r13')\n",
        "\n",
        "# Without providing additional arguments, the tokenizer will output undiacritized\n",
        "# morphological tokens for each input word delimited by an underscore.\n",
        "tokenizer = MorphologicalTokenizer(mle, scheme='d3tok')\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)\n",
        "\n",
        "# By specifying `split=True`, the morphological tokens are output as seperate\n",
        "# strings.\n",
        "tokenizer = MorphologicalTokenizer(mle, scheme='d3tok', split=True)\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)\n",
        "\n",
        "# We can output diacritized tokens by setting `diac=True`\n",
        "tokenizer = MorphologicalTokenizer(mle, scheme='d3tok', split=True, diac=True)\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['فتنفست', 'الصعداء']\n",
            "['ف+_تنفست', 'ال+_صعداء']\n",
            "['ف+', 'تنفست', 'ال+', 'صعداء']\n",
            "['فَ+', 'تَنَفَّسْتُ', 'ال+', 'صُعَداءَ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdCz7XnjELcm",
        "outputId": "32c5d025-a93d-46cf-bbdc-e669a85208e2"
      },
      "source": [
        "#Dialect Identification\n",
        "\n",
        "from camel_tools.dialectid import DialectIdentifier\n",
        "\n",
        "did = DialectIdentifier.pretrained()\n",
        "\n",
        "sentences = [\n",
        "    'مال الهوى و مالي شكون اللي جابني ليك  ما كنت انايا ف حالي بلاو قلبي يانا بيك',\n",
        "    'بدي دوب قلي قلي بجنون بحبك انا مجنون ما بنسى حبك يوم'\n",
        "]\n",
        "\n",
        "predictions = did.predict(sentences, 'city')\n",
        "print([p.top for p in predictions])\n",
        "\n",
        "predictions = did.predict(sentences, 'country')\n",
        "print([p.top for p in predictions])\n",
        "\n",
        "predictions = did.predict(sentences, 'region')\n",
        "print([p.top for p in predictions])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.23.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.23.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator FeatureUnion from version 0.23.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator MultinomialNB from version 0.23.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.23.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator OneVsRestClassifier from version 0.23.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['Rabat', 'Beirut']\n",
            "['Morocco', 'Lebanon']\n",
            "['Maghreb', 'Levant']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUwj5kwvDjV0",
        "outputId": "009e4ec2-181a-4ab6-d8ff-27a1b0b8ff71"
      },
      "source": [
        "#Sentiment Analysis\n",
        "from camel_tools.sentiment import SentimentAnalyzer\n",
        "\n",
        "sa = SentimentAnalyzer.pretrained()\n",
        "\n",
        "sentences = [\n",
        "    'أنا بخير',\n",
        "    'أنا لست بخير'\n",
        "]\n",
        "\n",
        "sentiments = sa.predict(sentences)\n",
        "\n",
        "print(sentiments)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['positive', 'negative']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}